**Just some thoughts as I go**
- Don't like Java, but I hope it's gonna be worth it :smiley: 
- Started this project on my own just to fill the gaps in my knowledge optimizers` inner workings and to finally play with Calcite as a nice bonus
- Calcite starter documentation is a crap IMO. I already had a pleasure of trying to understand how it works years ago when working at heavy.ai, but to no success. Unfortunately, it didn't get any better with years.
- As I understand, CMU students can get some help from TA and from each other. Myself, unfortunately, can only rely on search engines and github forks (as a last resort, hope not to go there)
- The most useful resource for building a query optimizer with Calcite so far turns out to be this blog https://www.querifylabs.com/blog as they show how to run planner with basic set of rules. They also occasionally go deeper into specific topics.
- The second most useful resource is Calcite unit tests. Can grab some building blocks from there
- For the beginning I've decided to go with JdbcSchema to read from duckdb. But it doesn't work correctly cuz it cannot understand DuckDB`s BASE TABLE type. Will have to write my own schema or add some hacks on top of JDBC schema
- I see that I can call Volcano planer in at least two ways: as a Calcite Program and as findBestExp. Not sure which should I use, just opted for the latter due to its simplicity. Probably Program is more flexible, as I understand
- After quite a bit of trying I managed to get a set of rules to execute my Jdbc physical nodes. Had some troubles with enumerable scan, still not sure what helped to get it going.
- Decided to commit my first iteration in this status:
- - 9 queries pass
- - 9 queries timeout
- - 8 queries fail, mostly due to `variable $cor0 is not found`
- I've decided to do statistics "the right way" - that is implementing my own schema and in-memory table type.
- The result is almost no queries are passing due to many unsupported conversions and type casts (still no idea why it tries to convert date to int in q7).
- Another weirdness - when I add unique columns stats Calcite gives me bogus SQL which is a) plain wrong; b) ambiguous. Had to forfeit this idea for now, though I expected it to be helpful in many rewrites and cost estimates.
- Kinda regret trying to write my own scannable table type. I could have made things much easier just by extending JdbcSchema with stats.
- At this point I still feel quite blind: docs still suck and everything looks like a bunch of random changes to see what works, without real understanding why.
- Things started to making sense, somewhat. I noticed that a bunch of queries failed to convert LogicalFilters and what's common is that they all have subqueries in filter inputs. I tried decorrellation intuitively and failed. Then I looked into Calcite Unit tests to look for examples of decorrelation and turns out that I first need to manually (i.e. with some core rules) convert subqueries to correlate nodes and only than I can decorrelate. Which kinda makes sense.
- Now more queries are passing, but there are some OOM. I wanna make sure that I'm pushing down filters properly now.
- My custom set of rules managed to execute 15 queries properly, but then I tried default rules w/o merge join and limit sort rule. And it seems to be almost good enough: 19 queries execute fine, 1 times out and 5 fail to execute enumerable limit and 1 exceed java heap size (OOM)
- Got all but two queries passing: q19 TO and q9 OOM. Just turned TopN rule back. Turned it down simply because someone in the Internat wrote that it doesn't work. Don't trust the Internet :smiley:
- Quick look at q19 shows that there is a very sub-optimal LogicalFilter like `(A and B and C) or (A and D and C) or (A and Z and C)`. And to my surprise whatever I do there seem to be no rules in Calcite to transform it to `A and C and (B or D or Z)`. Some googling, or rather GPTing cuz searching anything on Calcite is quite hard, shows that there is a RexNode pullFactors method that might help. As far as I understand I should implement some rule or operator myself
- Yep, writing a Rule to pull common factors from LogicalFilter turned out being quite trivial using RexNode.pullFactors
- After spending a while trying to figure out a proper way to fix OOMs in q9 and q21 I've decided to ditch it for now and try to submit the little that I have to gradescope. There was some struggle with gradescope as it had some expectations for java heap size, lack of missing files even if a query failed and some other staff. But I finally got my submission graded, for ~60.
- It turns out you can take a look at "reference solution"`s optimized plans and see what you miss.
- I still couldn't get any more optimizations after two evenings of struggle. I believe the biggest problem is that I took default preset of Calcite rules and try to play around it. It's a good starting point **BUT** it's hard to improve on it cuz you cannot possibly keep all the 100+ rules from the preset and thoughtfully improve on it.  
So I've decided to take a step back and start thoughtfully from the most nominalistic rule-set. Got score from ~60 to ~40
- Couldn't understand why my score was 0 for some tasks, so I've just downloaded grader.py. Turns out it gives 0 score if resulsts in Calcite don't match. And what a surprise - reference solution fails to convert sql.Date to LocalDate and gives exception for half of the tasks. So my solution is also expected to give an exception. However, turns out that it's smart enough to ignore this exception, I was just sloppy and created qX_results**.txt** instead of **.csv**. After changing to csv I've got my score up.
- I also noticed that bushy joins give very good plans sometimes. However, they significantly increase the search space. So, I've added an optimization task with bushy joins **and with timeout**. If it fails, I do a simpler join optimization. Now q9 and q21 pass and my score is ~93.
- Noticed, that gradescope results are somewhat unreliable. I can get scores from 70 to 111 for the same submission. As I can't rely on it I've decided to compete against myself - now I run the workload 5 times and report average times. Then I compare results from previous runs and current results with `python3 improvement_report.py`.
- Turns out that prepareStatement calls our planner once more. This is bad cuz apparently some plans can't be optimized twice (they just freeze). To fix it we can remove all rules but Enumerable implementation rules from our planner.
- Weird, but it seems like Calcite can go into infinite optimization loop if it has conflicting rules (e.g. projection pull up VS projection push down)
- Seems like multi join plans (including bushy joins) are optimized more heuristically than based on costs. Sometimes I get really bad plans (q9 for example). With somewhat random rule changes I've managed to get q9 working (due to timeout in bushy joins optimization)
- TIMEOUT in the first round of optimizations allows me to try much wider search space: associativity, commutativity, bushy joins. Looks like it helps for some queries.
- Removed bushy- and multi-joins: as this optimization is mostly heuristic it's hard to optimize further. I just can't control what rules are applied there base on costs and there are lots of Cartesian products. Removing bushy joins improved most of the queries (surprisingly it was the opposite before, when I had worse rules I suppose).
- Turned on statistics on unique columns. Doesn't look to be making any difference, but perhaps I will be able to manually improve cost estimates using it
- Things get much easier if you have a tool to compare with plans from previous run[s]
- Better support for associativity. I thought that it will automatically push join through join, but you actually need a separate rule for that
- Tried to convert join to semijoin for unique columns, but it generates bad SQL. Doesn't seem to be working
- Somewhat unnoticed to me was the fact that there are two costing implementations in Calcite: VolcanoCost (CPU, ROWS, IO) and RelOptsCostImpl (simpler single-value cost). I've used the second one while it turns out that VolcanoCost is the main costing algorithm. After switching to it I got somewhat better results. However even VolcanoCost still looks awfully bad.
- I tried reordering Filter conditions by their selectivity (or by their cost * selectivity, whatever). But to make it work I need it to be a cost-based rule. Which turns out a lot of work. Ditching this idea at least for now.
- I finally figured out that doing bushy joins w/o cycles and OOMs requires splitting optimization in two phases cuz `JOIN_TO_MULTI_JOIN` cant run in cost-based mode. Hence, I now have two-stage optimizer
- Sometimes bushy joins are better than hand-picked optimization ruleset w/o bushy joins. But most of the time they are not. I've decided to evaluate both ruleset and pick the best one (based on costs). Alas, cost formulas in Calcite suck (even though I define unique columns, it doesn't seem to help). 
- So I've decided to implement my own selectivity estimate for TableScan and EQUALS predicate. Also I've added NDV estimates for TableScans. Those stats seem to help somewhat, for example in query8: `EnumerableFilter(condition=[=($4, 'SMALL BRUSHED NICKEL')]): rowcount = 15000.0, cumulative cost = {115000.0 rows, 200001.0 cpu, 0.0 io}, id = 3699` - the default estimate and `EnumerableFilter(condition=[=($4, 'SMALL BRUSHED NICKEL')]): rowcount = 666.6666666666667, cumulative cost = {100666.66666666667 rows, 200001.0 cpu, 0.0 io}, id = 407432` - my estimate. However it still gives huge estimate errors. E.g. inner join in star schema of Q8 `lineitem join (parts crossjoin suppliers) on partkey, suppkey` should give **at most** #lineitem rows cuz it's a star-schema query, but gives `#lineitem X #crossjoin X selectivity` -> 1.8003645E12 instead of 6001215 rows.
- To fix the above I need either a heuristic for star schema or better cost-estimate for such queries.
- I've tried to implement better cost estimates for joining on unique keys (whether those keys come directly from tables or from another input join), but it turned out being a bad idea. While predictions improved for those cases, often other better plans were ignored cuz they lacked a similar cost estimate implementation. This leads to **worse** plans on average. So my conclusion is that you either have to implement a full range of cost estimates for specific equivalence class or don't do it at all.
- However, when debugging plans for Q8 I've noticed that Calcite constantly picks `A JOIN B` such that |B| is greater than |A|. This is not compute-efficient for DuckDB (and most other hash-join implementations). Just switching order makes Q8 much faster. A proper solution would be to implement better Join cost estimates, but it seems too much work and also VolcanoCost doesn't care much about CPU or IO costs (which matter for different join implementations and orders), Calcite only cares about row counts, which are always the same. So I've implemented a hack - I've just penalized such plans by a cost factor of 5 (i.e. `result rows = result rows * 5`. And this little hack improved performance of many queries dramatically. For example, Q8 became ~10 times faster.
- The above change already gave me good results at gradescope. About ~97 points vs Refsol`s 90.0. However I've also notices that my Q13 plan is overcomplicated and performs worse than expected due to Join condition pushdown rule. Removing it gave me another 4 points.
- And final improvement was a better selectivity estimate for `LIKE` and `NOT LIKE`
- Gradescope`s measurements are very unstable, but my final submissions get scores between 92 and 107 (usually closer to 102), which is significantly better than 90.8 Refsol baseline. Hence, I think it's time to stop optimizing my optimizer :) Though I might get back to it now and then when I want to try some interesting rules I read about. It might be fun to actually implement a HEP rule for star joins, or order conditions based on their promise, or perhaps add a better decorelation rule.
- As a final thought, it was a fun project but I didn't expect Calcite default cost model and cost estimates being that poor. Also Calcite documentation is quite bad.