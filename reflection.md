**Just some thoughts as I go**
- Don't like Java, but I hope it's gonna be worth it :smiley: 
- Started this project on my own just to fill the gaps in my knowledge optimizers` inner workings and to finally play with Calcite as a nice bonus
- Calcite starter documentation is a crap IMO. I already had a pleasure of trying to understand how it works years ago when working at heavy.ai, but to no success. Unfortunately, it didn't get any better with years.
- As I understand, CMU students can get some help from TA and from each other. Myself, unfortunately, can only rely on search engines and github forks (as a last resort, hope not to go there)
- The most useful resource for building a query optimizer with Calcite so far turns out to be this blog https://www.querifylabs.com/blog as they show how to run planner with basic set of rules. They also occasionally go deeper into specific topics.
- The second most useful resource is Calcite unit tests. Can grab some building blocks from there
- For the beginning I've decided to go with JdbcSchema to read from duckdb. But it doesn't work correctly cuz it cannot understand DuckDB`s BASE TABLE type. Will have to write my own schema or add some hacks on top of JDBC schema
- I see that I can call Volcano planer in at least two ways: as a Calcite Program and as findBestExp. Not sure which should I use, just opted for the latter due to its simplicity. Probably Program is more flexible, as I understand
- After quite a bit of trying I managed to get a set of rules to execute my Jdbc physical nodes. Had some troubles with enumerable scan, still not sure what helped to get it going.
- Decided to commit my first iteration in this status:
- - 9 queries pass
- - 9 queries timeout
- - 8 queries fail, mostly due to `variable $cor0 is not found`
- I've decided to do statistics "the right way" - that is implementing my own schema and in-memory table type.
- The result is almost no queries are passing due to many unsupported conversions and type casts (still no idea why it tries to convert date to int in q7).
- Another weirdness - when I add unique columns stats Calcite gives me bogus SQL which is a) plain wrong; b) ambiguous. Had to forfeit this idea for now, though I expected it to be helpful in many rewrites and cost estimates.
- Kinda regret trying to write my own scannable table type. I could have made things much easier just by extending JdbcSchema with stats.
- At this point I still feel quite blind: docs still suck and everything looks like a bunch of random changes to see what works, without real understanding why.
- Things started to making sense, somewhat. I noticed that a bunch of queries failed to convert LogicalFilters and what's common is that they all have subqueries in filter inputs. I tried decorrellation intuitively and failed. Then I looked into Calcite Unit tests to look for examples of decorrelation and turns out that I first need to manually (i.e. with some core rules) convert subqueries to correlate nodes and only than I can decorrelate. Which kinda makes sense.
- Now more queries are passing, but there are some OOM. I wanna make sure that I'm pushing down filters properly now.
- My custom set of rules managed to execute 15 queries properly, but then I tried default rules w/o merge join and limit sort rule. And it seems to be almost good enough: 19 queries execute fine, 1 times out and 5 fail to execute enumerable limit and 1 exceed java heap size (OOM)
- Got all but two queries passing: q19 TO and q9 OOM. Just turned TopN rule back. Turned it down simply because someone in the Internat wrote that it doesn't work. Don't trust the Internet :smiley:
- Quick look at q19 shows that there is a very sub-optimal LogicalFilter like `(A and B and C) or (A and D and C) or (A and Z and C)`. And to my surprise whatever I do there seem to be no rules in Calcite to transform it to `A and C and (B or D or Z)`. Some googling, or rather GPTing cuz searching anything on Calcite is quite hard, shows that there is a RexNode pullFactors method that might help. As far as I understand I should implement some rule or operator myself
- Yep, writing a Rule to pull common factors from LogicalFilter turned out being quite trivial using RexNode.pullFactors
- After spending a while trying to figure out a proper way to fix OOMs in q9 and q21 I've decided to ditch it for now and try to submit the little that I have to gradescope. There was some struggle with gradescope as it had some expectations for java heap size, lack of missing files even if a query failed and some other staff. But I finally got my submission graded, for ~60.
- It turns out you can take a look at "reference solution"`s optimized plans and see what you miss.
- I still couldn't get any more optimizations after two evenings of struggle. I believe the biggest problem is that I took default preset of Calcite rules and try to play around it. It's a good starting point **BUT** it's hard to improve on it cuz you cannot possibly keep all the 100+ rules from the preset and thoughtfully improve on it.  
So I've decided to take a step back and start thoughtfully from the most nominalistic rule-set. Got score from ~60 to ~40
- Couldn't understand why my score was 0 for some tasks, so I've just downloaded grader.py. Turns out it gives 0 score if resulsts in Calcite don't match. And what a surprise - reference solution fails to convert sql.Date to LocalDate and gives exception for half of the tasks. So my solution is also expected to give an exception. However, turns out that it's smart enough to ignore this exception, I was just sloppy and created qX_results**.txt** instead of **.csv**. After changing to csv I've got my score up.
- I also noticed that bushy joins give very good plans sometimes. However, they significantly increase the search space. So, I've added an optimization task with bushy joins **and with timeout**. If it fails, I do a simpler join optimization. Now q9 and q21 pass and my score is ~93.
- Noticed, that gradescope results are somewhat unreliable. I can get scores from 70 to 111 for the same submission. As I can't rely on it I've decided to compete against myself - now I run the workload 5 times and report average times. Then I compare results from previous runs and current results with `python3 improvement_report.py`.
- Turns out that prepareStatement calls our planner once more. This is bad cuz apparently some plans can't be optimized twice (they just freeze). To fix it we can remove all rules but Enumerable implementation rules from our planner.
- Weird, but it seems like Calcite can go into infinite optimization loop if it has conflicting rules (e.g. projection pull up VS projection push down)
- Seems like multi join plans (including bushy joins) are optimized more heuristically than based on costs. Sometimes I get really bad plans (q9 for example). With somewhat random rule changes I've managed to get q9 working (due to timeout in bushy joins optimization)
- TIMEOUT in the first round of optimizations allows me to try much wider search space: associativity, commutativity, bushy joins. Looks like it helps for some queries.